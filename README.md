# Adversarial Attack by Input Significance Indicator
A fast and efficient white-box iterative adversarial attack algorithm against deep learning models based on score backpropagation.

## Introduction
This repository contains the code for the real data experiments of the paper:
- [Generating Adversarial Examples with Input Significance Indicator](https://doi.org/10.1016/j.neucom.2020.01.040)

## Method
The adversarial examples are generated by extracting the pixels that are most relevant for the prediction of the model via later-wise relevance propagation, and then modify them with different strategies.

## Dependencies
- [tensorflow]
- [adversarial robustness toolbox](https://github.com/IBM/adversarial-robustness-toolbox) (for evaluation)
